{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10141292,"sourceType":"datasetVersion","datasetId":6259306,"isSourceIdPinned":false},{"sourceId":10141323,"sourceType":"datasetVersion","datasetId":6259335,"isSourceIdPinned":false},{"sourceId":10141416,"sourceType":"datasetVersion","datasetId":6259401,"isSourceIdPinned":false}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:39:57.975207Z","iopub.execute_input":"2024-12-08T18:39:57.975673Z","iopub.status.idle":"2024-12-08T18:39:57.989768Z","shell.execute_reply.started":"2024-12-08T18:39:57.975608Z","shell.execute_reply":"2024-12-08T18:39:57.988541Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vectorization/vectorization.py\n/kaggle/input/reviews/star_reviews (1).db\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import sqlite3\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:45:23.234698Z","iopub.execute_input":"2024-12-08T22:45:23.235188Z","iopub.status.idle":"2024-12-08T22:45:23.630502Z","shell.execute_reply.started":"2024-12-08T22:45:23.235148Z","shell.execute_reply":"2024-12-08T22:45:23.629600Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load in preprocessed sql database:\nfilename = '/kaggle/input/reviews/star_reviews (1).db'\nconn = sqlite3.connect(filename)\ncursor = conn.cursor()\n\nquery = \"SELECT * FROM data\"\ndf = pd.read_sql(query, conn)\nprint(df)\n\ncursor.close()\nconn.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:45:23.663809Z","iopub.execute_input":"2024-12-08T22:45:23.664244Z","iopub.status.idle":"2024-12-08T22:45:23.882681Z","shell.execute_reply.started":"2024-12-08T22:45:23.664215Z","shell.execute_reply":"2024-12-08T22:45:23.881741Z"}},"outputs":[{"name":"stdout","text":"       stars                                     processed_text\n0        3.0  decid eat here, awar go take two hour begin en...\n1        5.0  i'v taken lot spin class years, noth compar cl...\n2        3.0  famili dinner. buffets. eclect assortment: lar...\n3        5.0  now! mummy, different, delicious. favorit lamb...\n4        4.0  mute interior owner (?) gave us tour come rati...\n...      ...                                                ...\n26995    4.0  inn mari bar area wonderful. son love chees fi...\n26996    5.0  first review yelp feel share experience. move ...\n26997    3.0  place perfect want stay bourbon staff nice man...\n26998    3.0  aimlessli stare menu, get foggi larger! mean, ...\n26999    5.0  place delightful. came parent last saturday li...\n\n[27000 rows x 2 columns]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom sklearn.decomposition import TruncatedSVD\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:41:30.650811Z","iopub.execute_input":"2024-12-08T23:41:30.651176Z","iopub.status.idle":"2024-12-08T23:41:30.655856Z","shell.execute_reply.started":"2024-12-08T23:41:30.651146Z","shell.execute_reply":"2024-12-08T23:41:30.654842Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# import .py file\nimport sys\nsys.path.insert(1, '/kaggle/input/vectorization') \nsys.path.insert(1, '/kaggle/input/imports') \nimport vectorization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:45:26.409337Z","iopub.execute_input":"2024-12-08T22:45:26.410016Z","iopub.status.idle":"2024-12-08T22:45:40.600233Z","shell.execute_reply.started":"2024-12-08T22:45:26.409982Z","shell.execute_reply":"2024-12-08T22:45:40.599390Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# COMPLETE DATA\n# preprocessed database into vectorization module\nfrom vectorization import *\n\nxraw, yraw = vectorization.sql_query(filename)\n\ntrainX, testX, trainY, testY = vectorization.vectorize(filename)\ntrainY = trainY.astype(int)\ntestY = testY.astype(int)\n\n# Check sizes\nprint(trainX.shape, trainY.shape, testX.shape, testY.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:22:48.499923Z","iopub.execute_input":"2024-12-09T00:22:48.500265Z","iopub.status.idle":"2024-12-09T00:22:49.799687Z","shell.execute_reply.started":"2024-12-09T00:22:48.500233Z","shell.execute_reply":"2024-12-09T00:22:49.798676Z"}},"outputs":[{"name":"stdout","text":"(21600, 25366) (21600,) (5400, 25366) (5400,)\n","output_type":"stream"}],"execution_count":219},{"cell_type":"code","source":"# Perform dimensionality reduction to minimize computational time for CNN and make sparse matrices dense\n\nnum_components = 500\nsvd = TruncatedSVD(n_components=num_components)\ntrainX_red = svd.fit_transform(trainX)\ntestX_red = svd.fit_transform(testX)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:22:52.504843Z","iopub.execute_input":"2024-12-09T00:22:52.505581Z","iopub.status.idle":"2024-12-09T00:23:11.290356Z","shell.execute_reply.started":"2024-12-09T00:22:52.505546Z","shell.execute_reply":"2024-12-09T00:23:11.289627Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"Xtrain_tensor = torch.tensor(trainX_red, dtype=torch.float32) \nXtest_tensor = torch.tensor(testX_red, dtype=torch.float32)\nytrain_tensor = torch.tensor(trainY.values, dtype=torch.long)  \nytest_tensor = torch.tensor(testY.values, dtype=torch.long)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.291637Z","iopub.execute_input":"2024-12-09T00:23:11.291868Z","iopub.status.idle":"2024-12-09T00:23:11.308258Z","shell.execute_reply.started":"2024-12-09T00:23:11.291845Z","shell.execute_reply":"2024-12-09T00:23:11.307491Z"}},"outputs":[],"execution_count":221},{"cell_type":"code","source":"Xtrain_tensor = Xtrain_tensor.unsqueeze(1) # includes channels size \nXtest_tensor = Xtest_tensor.unsqueeze(1)\nytrain_tensor -= 1 # prepares for crossentropy classification expected input \nytest_tensor -= 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.309183Z","iopub.execute_input":"2024-12-09T00:23:11.309449Z","iopub.status.idle":"2024-12-09T00:23:11.314638Z","shell.execute_reply.started":"2024-12-09T00:23:11.309421Z","shell.execute_reply":"2024-12-09T00:23:11.313749Z"}},"outputs":[],"execution_count":222},{"cell_type":"code","source":"Xtrain_tensor.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.316630Z","iopub.execute_input":"2024-12-09T00:23:11.317008Z","iopub.status.idle":"2024-12-09T00:23:11.325057Z","shell.execute_reply.started":"2024-12-09T00:23:11.316977Z","shell.execute_reply":"2024-12-09T00:23:11.324143Z"}},"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"torch.Size([21600, 1, 500])"},"metadata":{}}],"execution_count":223},{"cell_type":"code","source":"# Create DataLoader for batching and shuffling\ntrain_dataset = TensorDataset(Xtrain_tensor, ytrain_tensor)\ntest_dataset = TensorDataset(Xtest_tensor, ytest_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) \ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.326092Z","iopub.execute_input":"2024-12-09T00:23:11.326414Z","iopub.status.idle":"2024-12-09T00:23:11.333343Z","shell.execute_reply.started":"2024-12-09T00:23:11.326370Z","shell.execute_reply":"2024-12-09T00:23:11.332603Z"}},"outputs":[],"execution_count":224},{"cell_type":"code","source":"class CNN(nn.Module):\n    '''\n    Convolutional neural network class with max pooling and fully connected layer\n    '''\n    def __init__(self, input_dim, num_classes, num_filters=200, input_channels=1):\n        super(CNN, self).__init__()\n\n        # Define convolutional layers\n        self.conv = nn.Conv2d(input_channels, num_filters, (1, input_dim))\n\n        # Fully connected layer\n        self.fc = nn.Linear(num_filters, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # channel dimension\n\n        # Apply each convolutional layer and pooling\n        \n        conv_out = self.conv(x)  \n        conv_out = F.relu(conv_out)  \n        pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1))  # Max pooling\n        pooled = pooled.squeeze(3)  # remove the last dimension\n        \n        # Flatten for the fully connected layer\n        x = pooled.view(pooled.size(0), -1)  # Flatten the tensor\n        x = self.fc(x)  # Fully connected layer\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.334428Z","iopub.execute_input":"2024-12-09T00:23:11.334692Z","iopub.status.idle":"2024-12-09T00:23:11.347349Z","shell.execute_reply.started":"2024-12-09T00:23:11.334668Z","shell.execute_reply":"2024-12-09T00:23:11.346512Z"}},"outputs":[],"execution_count":225},{"cell_type":"code","source":"def train_scores(model, train_loader, optimizer):\n    ''' Trains model with given data\n    Parameters: model (NN for our case), train_loader is the torch.DataLoader which provides data and labels,\n    optimizer is the torch.optimizer to update parameters\n    Returns: tuple of the avg train loss and train accuracy scores \n    '''\n    model.train()  \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, targets) in enumerate(train_loader):\n        \n        optimizer.zero_grad() # Zero the gradients\n        # Forward pass\n        outputs = model(data)\n        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n        loss = criterion(outputs, targets)\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        total += targets.size(0)\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = 100 * correct / total\n    \n    return train_loss, train_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.348221Z","iopub.execute_input":"2024-12-09T00:23:11.348491Z","iopub.status.idle":"2024-12-09T00:23:11.358676Z","shell.execute_reply.started":"2024-12-09T00:23:11.348467Z","shell.execute_reply":"2024-12-09T00:23:11.357962Z"}},"outputs":[],"execution_count":226},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    ''' Tests model on the testing data\n    Parameters: model (NN), test_loader is the DataLoader with test data and labels\n    Returns: tuple with test loss and test accuracy scores\n    '''\n    model.eval() \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # No need to track gradients during evaluation\n        for batch_idx, (data, targets) in enumerate(test_loader):\n            \n            # Forward pass\n            outputs = model(data)\n            criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(targets).sum().item()\n            total += targets.size(0)\n\n    test_loss = running_loss / len(test_loader)\n    test_accuracy = 100 * correct / total\n    return test_loss, test_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.359814Z","iopub.execute_input":"2024-12-09T00:23:11.360549Z","iopub.status.idle":"2024-12-09T00:23:11.370408Z","shell.execute_reply.started":"2024-12-09T00:23:11.360511Z","shell.execute_reply":"2024-12-09T00:23:11.369662Z"}},"outputs":[],"execution_count":227},{"cell_type":"code","source":"# Initialize model\nmodel = CNN(input_dim=num_components, num_classes=5)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Tested different learning rates, .001 seems best\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.371507Z","iopub.execute_input":"2024-12-09T00:23:11.372016Z","iopub.status.idle":"2024-12-09T00:23:11.380281Z","shell.execute_reply.started":"2024-12-09T00:23:11.371980Z","shell.execute_reply":"2024-12-09T00:23:11.379477Z"}},"outputs":[],"execution_count":228},{"cell_type":"code","source":"num_epochs = 20\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    \n    # Train the model\n    train_loss, train_accuracy = train_scores(model, train_loader, optimizer)\n    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n\n    # Evaluate the model\n    test_loss, test_accuracy = evaluate(model, test_loader)\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:23:11.382094Z","iopub.execute_input":"2024-12-09T00:23:11.382330Z","iopub.status.idle":"2024-12-09T00:23:33.721591Z","shell.execute_reply.started":"2024-12-09T00:23:11.382308Z","shell.execute_reply":"2024-12-09T00:23:33.720661Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\nTraining Loss: 1.2606, Training Accuracy: 52.34%\nTest Loss: 1.7333, Test Accuracy: 38.31%\n\nEpoch 2/20\nTraining Loss: 1.0981, Training Accuracy: 61.25%\nTest Loss: 1.8057, Test Accuracy: 37.78%\n\nEpoch 3/20\nTraining Loss: 1.0591, Training Accuracy: 63.63%\nTest Loss: 1.8391, Test Accuracy: 37.56%\n\nEpoch 4/20\nTraining Loss: 1.0241, Training Accuracy: 66.10%\nTest Loss: 1.8887, Test Accuracy: 36.00%\n\nEpoch 5/20\nTraining Loss: 0.9862, Training Accuracy: 68.86%\nTest Loss: 1.9217, Test Accuracy: 35.46%\n\nEpoch 6/20\nTraining Loss: 0.9445, Training Accuracy: 72.19%\nTest Loss: 1.9772, Test Accuracy: 35.65%\n\nEpoch 7/20\nTraining Loss: 0.9003, Training Accuracy: 75.25%\nTest Loss: 2.0070, Test Accuracy: 35.41%\n\nEpoch 8/20\nTraining Loss: 0.8531, Training Accuracy: 78.80%\nTest Loss: 2.0670, Test Accuracy: 34.39%\n\nEpoch 9/20\nTraining Loss: 0.8082, Training Accuracy: 81.76%\nTest Loss: 2.0914, Test Accuracy: 35.31%\n\nEpoch 10/20\nTraining Loss: 0.7650, Training Accuracy: 84.51%\nTest Loss: 2.1231, Test Accuracy: 34.57%\n\nEpoch 11/20\nTraining Loss: 0.7257, Training Accuracy: 86.97%\nTest Loss: 2.1843, Test Accuracy: 34.09%\n\nEpoch 12/20\nTraining Loss: 0.6898, Training Accuracy: 89.20%\nTest Loss: 2.2241, Test Accuracy: 34.17%\n\nEpoch 13/20\nTraining Loss: 0.6578, Training Accuracy: 91.02%\nTest Loss: 2.2470, Test Accuracy: 34.65%\n\nEpoch 14/20\nTraining Loss: 0.6299, Training Accuracy: 92.69%\nTest Loss: 2.2957, Test Accuracy: 34.24%\n\nEpoch 15/20\nTraining Loss: 0.6051, Training Accuracy: 93.79%\nTest Loss: 2.3365, Test Accuracy: 34.07%\n\nEpoch 16/20\nTraining Loss: 0.5837, Training Accuracy: 95.07%\nTest Loss: 2.3472, Test Accuracy: 33.87%\n\nEpoch 17/20\nTraining Loss: 0.5646, Training Accuracy: 95.86%\nTest Loss: 2.3805, Test Accuracy: 34.04%\n\nEpoch 18/20\nTraining Loss: 0.5479, Training Accuracy: 96.63%\nTest Loss: 2.4122, Test Accuracy: 33.57%\n\nEpoch 19/20\nTraining Loss: 0.5333, Training Accuracy: 97.26%\nTest Loss: 2.4342, Test Accuracy: 33.74%\n\nEpoch 20/20\nTraining Loss: 0.5207, Training Accuracy: 97.85%\nTest Loss: 2.4794, Test Accuracy: 33.80%\n\n","output_type":"stream"}],"execution_count":229},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
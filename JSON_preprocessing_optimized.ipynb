{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eitan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from num2words import num2words  # Converting numbers to word form\n",
    "import ijson  # Parser for large json files\n",
    "import json\n",
    "\n",
    "import nltk  # Import nltk module\n",
    "from nltk.corpus import stopwords  # Stop word dictionary\n",
    "from nltk.stem import PorterStemmer  # Stems words\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sqlite3 # Import sqlite to store data at end to sql database\n",
    "\n",
    "import os # For obtaining local directory to store file\n",
    "\n",
    "# Download stopwords dictionary\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Example of a json object in the review file:\n",
    "# {\"review_id\":\"btHrA_nXUceLqZRvymvXng\",\"user_id\":\"amaOELCfgLup2MwE2j8PfA\",\"business_id\":\"_v3DcLatG70adfYzWTd-CQ\",\"stars\":5.0,\"useful\":2,\"funny\":2,\"cool\":1,\"text\":\"I love this store!\",\"date\":\"2015-03-18 21:09:07\"}\n",
    "\n",
    "# Order of columns:\n",
    "#\"review_id\", \"user_id\", \"business_id\", \"stars\",\"useful\",\"funny\",\"cool\",\"text\",\"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cleaning/preprocessing function\n",
    "def text_preprocess(text):\n",
    "    '''\n",
    "    Function: Process takes an incoming JSON text review and preprocesses it by:\n",
    "    1. converting string to lowercase\n",
    "    2. removing stop words (is, it, a, but etc)\n",
    "    3. converts numbers to their word form (1 -> one) NOTE: Unclear if this is neccessary, but was suggsted online\n",
    "    4. Stem the words (convert words like programming -> program)\n",
    "    \n",
    "    Input: JSON item \"review\"\n",
    "    Output: preprocessed string\n",
    "    '''\n",
    "    # Define array to store processed words:\n",
    "    processed_words = []\n",
    "\n",
    "    # Define stop_words dict using nltk package\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Correct the string with TextBlob spell checker\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    text = str(blob.correct())\n",
    "\n",
    "    # Split string into list\n",
    "    split = text.split()\n",
    "    \n",
    "    # Create stemmer object\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Iterate through each word\n",
    "    for word in split:\n",
    "        # Convert word to lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        # Check if word is a stop word before proceeding\n",
    "        if word in stop_words:\n",
    "            continue # Continue will skip current iteration (current word) if it is a stop word\n",
    "        \n",
    "        if word == 'infinity': # Edgecase check if word is infinity, as num2words does not handle it correctly\n",
    "            processed_words.append('infinity')\n",
    "            continue # Continue to next word\n",
    "\n",
    "        # Convert numbers to word form \n",
    "        try:\n",
    "            # Check if word is a number\n",
    "            float(word)  # This will raise ValueError if the word is not a number\n",
    "            word = num2words(word)\n",
    "        except ValueError:\n",
    "            # If it's not a number, we can stem the word\n",
    "            word = ps.stem(word)\n",
    "        \n",
    "        processed_words.append(word)\n",
    "\n",
    "    # Join the processed words to form a clean text\n",
    "    clean_text = ' '.join(processed_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(cursor_all, cursor_stars):\n",
    "    \"\"\"\n",
    "    Create tables in the respective SQLite databases\n",
    "    Inputs: cursor_all, cursor_stars: Prefined SQLIte databases.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Create table for all_features.db\n",
    "    cursor_all.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS data (\n",
    "            review_id TEXT PRIMARY KEY,\n",
    "            user_id TEXT,\n",
    "            business_id TEXT,\n",
    "            stars REAL,\n",
    "            useful REAL,\n",
    "            funny REAL,\n",
    "            cool REAL,\n",
    "            text TEXT,\n",
    "            processed_text TEXT,\n",
    "            date TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Create table for star_reviews.db\n",
    "    cursor_stars.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS data (\n",
    "            stars REAL,\n",
    "            processed_text TEXT\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(cursor_all, cursor_stars, review):\n",
    "    \"\"\"\n",
    "    Inserts a single review into the SQLite databases\n",
    "    Inputs: cursor_all, cursor_stars: the SQLIte databases, containing all features, and just stars/reviews\n",
    "            review: the current JSON file line being procesed and inserted\n",
    "    Returns: None, but appends line to SQLIte databses\n",
    "    \"\"\"\n",
    "    review_id = review.get('review_id')\n",
    "    user_id = review.get('user_id')\n",
    "    business_id = review.get('business_id')\n",
    "    stars = review.get('stars')\n",
    "    stars = float(stars)\n",
    "\n",
    "    useful = review.get('useful')\n",
    "    funny = review.get('funny')\n",
    "    cool = review.get('cool')\n",
    "    text = review.get('text', '')\n",
    "    processed_text = text_preprocess(text)\n",
    "    date = review.get('date')\n",
    "\n",
    "    # Insert into all_features.db\n",
    "    cursor_all.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO data \n",
    "        (review_id, user_id, business_id, stars, useful, funny, cool, text, processed_text, date)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (review_id, user_id, business_id, stars, useful, funny, cool, text, processed_text, date))\n",
    "\n",
    "    # Insert into star_reviews.db\n",
    "    cursor_stars.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO data \n",
    "        (stars, processed_text)\n",
    "        VALUES (?, ?)\n",
    "    \"\"\", (stars, processed_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1500 records.\n",
      "Inserted 3000 records.\n",
      "Inserted 4500 records.\n",
      "Inserted 6000 records.\n",
      "Inserted 7500 records.\n",
      "Inserted 9000 records.\n",
      "Inserted 10500 records.\n",
      "Inserted 12000 records.\n",
      "Inserted 13500 records.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eitan/Documents/Supervised ML/FInal Project/JSON_preprocessing.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m objects:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(review, \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         insert_data(cursor_all, cursor_stars, review)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m# Commit every batch_size records\u001b[39;00m\n",
      "\u001b[1;32m/Users/eitan/Documents/Supervised ML/FInal Project/JSON_preprocessing.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cool \u001b[39m=\u001b[39m review\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcool\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m text \u001b[39m=\u001b[39m review\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m processed_text \u001b[39m=\u001b[39m text_preprocess(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m date \u001b[39m=\u001b[39m review\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Insert into all_features.db\u001b[39;00m\n",
      "\u001b[1;32m/Users/eitan/Documents/Supervised ML/FInal Project/JSON_preprocessing.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Correct the string with TextBlob spell checker\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m blob \u001b[39m=\u001b[39m TextBlob(text)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(blob\u001b[39m.\u001b[39;49mcorrect())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Split string into list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eitan/Documents/Supervised%20ML/FInal%20Project/JSON_preprocessing.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m split \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/blob.py:549\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mregexp_tokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+|[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    548\u001b[0m corrected \u001b[39m=\u001b[39m (Word(w)\u001b[39m.\u001b[39mcorrect() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens)\n\u001b[0;32m--> 549\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(corrected)\n\u001b[1;32m    550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/blob.py:548\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[1;32m    547\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mregexp_tokenize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+|[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 548\u001b[0m corrected \u001b[39m=\u001b[39m (Word(w)\u001b[39m.\u001b[39;49mcorrect() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens)\n\u001b[1;32m    549\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(corrected)\n\u001b[1;32m    550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/blob.py:115\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m Word(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspellcheck()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/blob.py:107\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspellcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[39m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m suggest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/en/__init__.py:118\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(w):\n\u001b[1;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a list of (word, confidence)-tuples of spelling corrections.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m spelling\u001b[39m.\u001b[39;49msuggest(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/_text.py:1692\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misdigit():\n\u001b[1;32m   1688\u001b[0m     \u001b[39mreturn\u001b[39;00m [(w, \u001b[39m1.0\u001b[39m)]  \u001b[39m# 1.5\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m candidates \u001b[39m=\u001b[39m (\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known([w])\n\u001b[1;32m   1691\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w))\n\u001b[0;32m-> 1692\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit2(w))\n\u001b[1;32m   1693\u001b[0m     \u001b[39mor\u001b[39;00m [w]\n\u001b[1;32m   1694\u001b[0m )\n\u001b[1;32m   1695\u001b[0m candidates \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(c, \u001b[39m0.0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m   1696\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(p \u001b[39mfor\u001b[39;00m p, word \u001b[39min\u001b[39;00m candidates) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/_text.py:1667\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/_text.py:1667\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/textblob/_text.py:103\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy(\u001b[39m\"\u001b[39m\u001b[39m__iter__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy(\u001b[39m\"\u001b[39m\u001b[39m__contains__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define input file name and Sqlite databses\n",
    "input_file = \"yelp_academic_dataset_review.json\"\n",
    "all_data_db = \"all_features.db\"\n",
    "star_reviews_db = \"star_reviews.db\"\n",
    "\n",
    "# Connect to SQLite databases\n",
    "conn_all = sqlite3.connect(all_data_db)\n",
    "conn_stars = sqlite3.connect(star_reviews_db)\n",
    "\n",
    "cursor_all = conn_all.cursor()\n",
    "cursor_stars = conn_stars.cursor()\n",
    "\n",
    "# Create tables\n",
    "create_tables(cursor_all, cursor_stars)\n",
    "\n",
    " # Open and parse the JSON file using ijson with multiple_values=True\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    # Initialize ijson parser for multiple top-level JSON objects\n",
    "    objects = ijson.items(f, '', multiple_values=True)\n",
    "    batch_size = 1500\n",
    "    count = 0\n",
    "    for review in objects:\n",
    "        if isinstance(review, dict):\n",
    "            insert_data(cursor_all, cursor_stars, review)\n",
    "            count += 1\n",
    "\n",
    "            # Commit every batch_size records\n",
    "            if count % batch_size == 0:\n",
    "                conn_all.commit()\n",
    "                conn_stars.commit()\n",
    "                print(f\"Inserted {count} records.\")\n",
    "\n",
    "\n",
    "# Commit all transactions\n",
    "conn_all.commit()\n",
    "conn_stars.commit()\n",
    "\n",
    "# Close database connections\n",
    "conn_all.close()\n",
    "conn_stars.close()\n",
    "\n",
    "print(\"Data insertion complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

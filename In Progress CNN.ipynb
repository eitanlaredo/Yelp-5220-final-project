{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b04793-0c87-4c5d-ad55-fc76480b0db3",
   "metadata": {},
   "source": [
    "## CNN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edee0dfc-4e05-4080-9df2-1a523b944c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef43a4f1-5d84-455e-8b99-c66e888ede50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stars                                     processed_text\n",
      "0        3.0  decid eat here, awar go take two hour begin en...\n",
      "1        5.0  i'v taken lot spin class years, noth compar cl...\n",
      "2        3.0  famili dinner. buffets. eclect assortment: lar...\n",
      "3        5.0  now! mummy, different, delicious. favorit lamb...\n",
      "4        4.0  mute interior owner (?) gave us tour come rati...\n",
      "...      ...                                                ...\n",
      "26995    4.0  inn mari bar area wonderful. son love chees fi...\n",
      "26996    5.0  first review yelp feel share experience. move ...\n",
      "26997    3.0  place perfect want stay bourbon staff nice man...\n",
      "26998    3.0  aimlessli stare menu, get foggi larger! mean, ...\n",
      "26999    5.0  place delightful. came parent last saturday li...\n",
      "\n",
      "[27000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load in preprocessed sql database:\n",
    "\n",
    "conn = sqlite3.connect('star_reviews.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT * FROM data\"\n",
    "df = pd.read_sql(query, conn)\n",
    "print(df)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e09041-5228-464a-9475-a680dba63a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nadia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text\n",
    "import vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd495903-43df-48d1-af05-773e56f9d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 25366) (21600,) (5400, 25366) (5400,)\n"
     ]
    }
   ],
   "source": [
    "# preprocessed database into vectorization module\n",
    "\n",
    "xraw, yraw = vectorization.sql_query('star_reviews.db')\n",
    "\n",
    "trainX, testX, trainY, testY = vectorization.vectorize('star_reviews.db')\n",
    "trainY = trainY.astype(int)\n",
    "testY = testY.astype(int)\n",
    "\n",
    "# Check sizes\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac54912-ebdd-40d7-8c36-92bbac883991",
   "metadata": {},
   "source": [
    "Turn vectorized data to tensors for pytorch usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e79af4-01a2-4d6b-bf19-7863b224625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe243326-b0f6-4250-abc9-bb3506ec3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction to minimize computational time for CNN, make sparse matrices dense\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f762f4c-a084-45c2-8d13-2955b9e1d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 300\n",
    "svd = TruncatedSVD(n_components=num_components)\n",
    "trainX_red = svd.fit_transform(trainX)\n",
    "testX_red = svd.fit_transform(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f401d59a-bc24-4941-b8f1-32f23b12a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6228150921655199\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_.sum())  # Check cumulative variance for choosing n_components above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d00a47-e54c-464c-8638-c831af5fb924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 400) (21600, 400)\n"
     ]
    }
   ],
   "source": [
    "print(testX_red.shape, trainX_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c828a1-839d-447e-a901-88a0bb342756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch tensors \n",
    "Xtrain_tensor = torch.tensor(trainX_red, dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c35cb-0d53-4d2d-9296-11c9a4552bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i have issues with this cell\n",
    "Xtest_tensor = torch.tensor(testX_red, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b358d0-4454-4493-8168-b667ccd81ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of zeros: 0.0\n"
     ]
    }
   ],
   "source": [
    "Xtest_sample = testX_red[:100]\n",
    "Xtest_sample = Xtest_sample.astype(np.float32)\n",
    "\n",
    "print(\"Proportion of zeros:\", np.sum(Xtest_sample == 0) / Xtest_sample.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1fc64-b9f3-490b-bb54-aac1718bdaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# testX_red = testX_red.astype('float32')\n",
    "Xtest_sample = testX_red[:100]\n",
    "Xtest_sample = Xtest_sample.astype(np.float32)\n",
    "\n",
    "# Verify the data type\n",
    "print(Xtest_sample.dtype)  \n",
    "\n",
    "Xtestsample_tensor = torch.tensor(Xtest_sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cfbe5-f095-4352-959e-e927f35cf9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_tensor = torch.tensor(trainY, dtype=torch.long)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e9e44-76d7-48d6-81a2-8e6b37903ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_tensor = torch.tensor(testY, dtype=torch.long)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4507002-b275-48b3-9310-ad0efc38c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching and shuffling\n",
    "train_dataset = TensorDataset(Xtrain_tensor, ytrain_tensor)\n",
    "test_dataset = TensorDataset(Xtest_tensor, ytest_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # batch size is the number of tokens considered\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be435190-76d0-4a60-b38b-e4b0d35c0789",
   "metadata": {},
   "source": [
    "### Define Convolutional neural network with torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b87c8c4-c373-458a-9e41-b2631f7cf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D attempt\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional neural network class with max pooling and fully connected layer\n",
    "    '''\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(64 * ((input_dim - 2 * 3) // 2), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Conv1\n",
    "        x = self.pool(x)  # Max pooling\n",
    "        # Second convolution and ReLU activation\n",
    "        x = F.relu(self.conv2(x))  # Conv2\n",
    "        x = self.pool(x)  # Max pooling\n",
    "        \n",
    "        # Flatten the output of the convolutional layers for fc layer\n",
    "        x = x.view(-1, 64 * ((x.size(2) - 2 * 3) // 2))  # Flatten\n",
    "        x = self.fc(x) # Fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49c60aa0-73b0-4eb7-a82c-592e621e50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional neural network class with max pooling and fully connected layer\n",
    "    '''\n",
    "    def __init__(self, input_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=100, input_channels=1):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define convolutional layers for different kernel sizes\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(input_channels, num_filters, (k, input_dim)) for k in kernel_sizes]\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to match Conv2d requirements\n",
    "        # Input shape: (batch_size, sequence_length, feature_dim)\n",
    "        # Reshaped to: (batch_size, input_channels, sequence_length, feature_dim)\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension (assuming input_channels=1)\n",
    "\n",
    "        # Apply each convolutional layer and pooling\n",
    "        conv_results = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = conv(x)  # Apply convolution\n",
    "            conv_out = F.relu(conv_out)  # Apply ReLU activation\n",
    "            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1))  # Max pooling\n",
    "            conv_results.append(pooled.squeeze(3))  # Remove the last dimension (size 1)\n",
    "\n",
    "        # Concatenate all results from different kernels\n",
    "        x = torch.cat(conv_results, 1)  # Shape: (batch_size, num_filters * len(kernel_sizes))\n",
    "\n",
    "        # Flatten for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe54c59-ce91-4b05-842a-f988f7d2cf82",
   "metadata": {},
   "source": [
    "# more comments\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional neural network class with max pooling and fully connected layer\n",
    "    '''\n",
    "    def __init__(self, input_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=100):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define convolutional layers for different kernel sizes\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (k, input_dim)) for k in kernel_sizes])\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add an extra dimension for channel (needed for Conv2d)\n",
    "        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length, feature_dim)\n",
    "\n",
    "        # Apply each convolutional layer and pooling\n",
    "        conv_results = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = conv(x)  # Shape: (batch_size, num_filters, sequence_length - kernel_size + 1, 1)\n",
    "            conv_out = F.relu(conv_out)\n",
    "            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1))  # Max pooling\n",
    "            conv_results.append(pooled.squeeze(3))  # Remove the last dimension (size 1)\n",
    "\n",
    "        # Concatenate all results from different kernels\n",
    "        x = torch.cat(conv_results, 1)  # Shape: (batch_size, num_filters * len(kernel_sizes))\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten for FC layer\n",
    "        x = self.fc(x)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647a03e8-0800-45dd-9cef-e2408187a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize CNN as model\n",
    "input_dim = num_components \n",
    "num_classes = 5  # Number of classes (stars)\n",
    "\n",
    "model = CNN(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "137f765a-d9a2-45cb-90f1-9a6a4f393eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation functions (dependent on tensors/trainloaders)\n",
    "def train_scores(model, train_loader, optimizer):\n",
    "    ''' Trains model with given data\n",
    "    Parameters: model (NN for our case), train_loader is the torch.DataLoader which provides data and labels,\n",
    "    optimizer is the torch.optimizer to update parameters\n",
    "    Returns: tuple of the avg train loss and train accuracy scores \n",
    "    '''\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        print(f\"Batch {batch_idx + 1}\")  # Print batch number to track progress\n",
    "        \n",
    "        print('data floated')\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        print('zero gradients')\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        print('outputs')\n",
    "        loss = nn.CrossEntropyLoss(outputs, targets)\n",
    "        print('loss calculated')\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    ''' Tests model on the testing data\n",
    "    Parameters: model (NN), test_loader is the DataLoader with test data and labels\n",
    "    Returns: tuple with test loss and test accuracy scores\n",
    "    '''\n",
    "    model.eval() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "            print(f\"Test Batch {batch_idx + 1}\")  # Print batch number for tracking\n",
    "            # data = data.float()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = nn.CrossEntropyLoss(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a35eed0-e0eb-41c7-a315-edc6cedc8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96dc3081-4c3f-459a-873a-1d63bb924929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21600, 1, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare tensor for CNN input\n",
    "Xtrain_tensor = Xtrain_tensor.unsqueeze(1)  # Now shape is [21600, 1, 800]\n",
    "Xtrain_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da3afe8f-24ae-45e7-95ba-bc4bdeb3be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_subset = Xtrain_tensor[:int(0.1 * Xtrain_tensor.shape[0])] # trying 10% sample of data to see if size is the bottleneck \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6be5b7-7b56-417f-83b2-e161966c0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model with Xtrain \n",
    "model = CNN(input_dim=num_components, num_classes=5)\n",
    "output = model(Xtrain_subset)\n",
    "print(output.shape)  # Expected output: (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd7116-0a14-4bf6-bfaa-7cc7e45089e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, train_accuracy = train_scores(model, train_loader, optimizer)\n",
    "    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b94b39-609e-4fa3-8436-a7c4c2f91616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 338\n",
      "Number of test batches: 85\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55077219-864d-4ecb-aa36-4adb36b6f760",
   "metadata": {},
   "source": [
    "## Testing with random data to see if corrupt file is issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20eb819c-569a-4882-b3c1-df5087762b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: tcbm wfurtptzidzxeh, Label: 1\n",
      "Text: tpbwdbpz odilivssocjk, Label: 4\n",
      "Text: nnougt, Label: 3\n",
      "Text: cailjujnycz blqvguhruayrdnljilyhegj gpzmzen, Label: 3\n",
      "Text: tgob he easq mynsp y r jkzlflbqrsdcei, Label: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Function to generate random text data\n",
    "def generate_random_text(num_samples=100, max_length=50):\n",
    "    texts = []\n",
    "    for _ in range(num_samples):\n",
    "        length = random.randint(5, max_length)  # Random length for each text\n",
    "        text = ''.join(random.choices(string.ascii_lowercase + ' ', k=length))  # Random text with lowercase letters and spaces\n",
    "        texts.append(text.strip())\n",
    "    return texts\n",
    "\n",
    "# Generate random labels (between 1 and 5) for each sample\n",
    "def generate_random_labels(num_samples=100):\n",
    "    labels = [random.randint(1, 5) for _ in range(num_samples)]\n",
    "    return labels\n",
    "\n",
    "# Generate a small random dataset\n",
    "num_samples = 100\n",
    "texts = generate_random_text(num_samples)\n",
    "labels = generate_random_labels(num_samples)\n",
    "\n",
    "# Display the first few examples\n",
    "for i in range(5):\n",
    "    print(f\"Text: {texts[i]}, Label: {labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c46b555-edbc-4779-b9d5-c343f7dc55da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 190), y shape: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the random text data\n",
    "vectorizer = TfidfVectorizer(max_features=800)  # You can adjust the number of features (dimensions)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# Now you have your text data (X) and labels (y) to pass to the model\n",
    "y = labels\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"X shape: {X.shape}, y shape: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e77b35-d64d-4008-a0b0-e27b92cff88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "X_tensor = X_tensor.unsqueeze(1)  # Adds a channel dimension, making the shape [100, 1, 184]\n",
    "\n",
    "# Convert labels to tensor (assuming y is already a list or array)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22088d24-93db-46f2-9b65-2099b8878f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(input_dim=184, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bd4bf-9974-4992-a0a1-e1c8a22bd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e5507-caa8-401c-9c36-a471ad4d8bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (Ali)",
   "language": "python",
   "name": "ali_exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

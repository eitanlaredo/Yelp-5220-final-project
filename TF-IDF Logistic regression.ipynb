{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/eitan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import onnxruntime as rt \n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = sql_vectorized(db_file=file_name, query_limit=100000)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "trainX_scaled = scaler.fit_transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.6476\n",
      "F1 Macro: 0.5608296542359805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_name = 'star_reviews.db'\n",
    "X_raw, Y_raw = sql_query_raw(db_file=file_name, query_limit=100000)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "raw_trainX, raw_testX, raw_trainY, raw_testY = train_test_split(X_raw, Y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Convert raw text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(raw_trainX)  # Fit on training data\n",
    "X_tfidf_test = tfidf_vectorizer.transform(raw_testX)       # Transform test data using the same vectorizer\n",
    "\n",
    "# Step 2: Train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_tfidf_train, raw_trainY)  # Train on training data and labels\n",
    "\n",
    "# Step 3: Make predictions on the test set\n",
    "pred_labels = log_reg.predict(X_tfidf_test)\n",
    "\n",
    "# Step 4: Evaluate the model using F1 scores\n",
    "f1_micro = f1_score(raw_testY, pred_labels, average=\"micro\") \n",
    "f1_macro = f1_score(raw_testY, pred_labels, average=\"macro\")\n",
    "\n",
    "print(f\"F1 Micro: {f1_micro}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.63455\n",
      "F1 Macro: 0.5492665574986099\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    random_state=SEED, solver=\"saga\", max_iter=5000, multi_class='ovr'\n",
    ")\n",
    "\n",
    "log_reg.fit(trainX, trainY)\n",
    "\n",
    "# Predict on the test set\n",
    "pred_labels = log_reg.predict(testX)\n",
    "\n",
    "# F1 scores for multiclass classification\n",
    "f1_micro = f1_score(testY, pred_labels, average=\"micro\")\n",
    "f1_macro = f1_score(testY, pred_labels, average=\"macro\")\n",
    "\n",
    "print(f\"F1 Micro: {f1_micro}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "\n",
    "# F1 Micro: 0.702125\n",
    "# F1 Macro: 0.6498956429594761\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.64075\n",
      "F1 Macro: 0.56679123342431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    random_state=SEED, solver=\"saga\", max_iter=10000, n_jobs=-1\n",
    ")\n",
    "log_reg.fit(trainX, trainY)\n",
    "\n",
    "pred_labels = log_reg.predict(testX)\n",
    "f1_micro = f1_score(testY, pred_labels, average=\"micro\")\n",
    "f1_macro = f1_score(testY, pred_labels, average=\"macro\")\n",
    "\n",
    "print(f\"F1 Micro: {f1_micro}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "\n",
    "# F1 Micro: 0.708125\n",
    "# F1 Macro: 0.6640736324269323"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
